{
 "metadata": {


  "signature": "sha256:307617176aa60c1ca97e856398fdfe4d0c1606e5a25f762b6662771b9112997d"

  "name": "Editing model parameters",
  "description": "How to do net surgery and manually change model parameters, making a fully-convolutional classifier for dense feature extraction.",
  "include_in_docs": true

 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Net Surgery\n",
      "\n",
      "Caffe models can be transformed to your particular needs by editing the network parameters. In this example, we translate the inner product classifier layers of the Caffe Reference ImageNet model into convolutional layers. This yields a fully-convolutional model that generates a classification map for any given input size instead of a single classification. In particular, a classification will be made for every 6 $\\times$ 6 region of the `pool5` layer, giving a 8 $\\times$ 8 classification map for our example 454 $\\times$ 454 input dimensions.\n",
      "\n",
      "Note that this model isn't totally appropriate for sliding-window detection since it was trained for whole-image classification. Sliding-window training and finetuning can be done by defining a sliding-window ground truth and loss such that a loss map is made for every location and solving as usual. (While planned, this is currently an exercise for the reader.)\n",
      "\n",
      "Roll up your sleeves for net surgery with pycaffe!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!diff imagenet/imagenet_full_conv.prototxt imagenet/imagenet_deploy.prototxt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1c1\r\n",
        "< name: \"CaffeNetConv\"\r\n",
        "---\r\n",
        "> name: \"CaffeNet\"\r\n",
        "3c3\r\n",
        "< input_dim: 1\r\n",
        "---\r\n",
        "> input_dim: 10\r\n",
        "5,6c5,6\r\n",
        "< input_dim: 454\r\n",
        "< input_dim: 454\r\n",
        "---\r\n",
        "> input_dim: 227\r\n",
        "> input_dim: 227\r\n",
        "151,152c151,152\r\n",
        "<   name: \"fc6-conv\"\r\n",
        "<   type: CONVOLUTION\r\n",
        "---\r\n",
        ">   name: \"fc6\"\r\n",
        ">   type: INNER_PRODUCT\r\n",
        "154,155c154,155\r\n",
        "<   top: \"fc6-conv\"\r\n",
        "<   convolution_param {\r\n",
        "---\r\n",
        ">   top: \"fc6\"\r\n",
        ">   inner_product_param {\r\n",
        "157d156\r\n",
        "<     kernel_size: 6\r\n",
        "163,164c162,163\r\n",
        "<   bottom: \"fc6-conv\"\r\n",
        "<   top: \"fc6-conv\"\r\n",
        "---\r\n",
        ">   bottom: \"fc6\"\r\n",
        ">   top: \"fc6\"\r\n",
        "169,170c168,169\r\n",
        "<   bottom: \"fc6-conv\"\r\n",
        "<   top: \"fc6-conv\"\r\n",
        "---\r\n",
        ">   bottom: \"fc6\"\r\n",
        ">   top: \"fc6\"\r\n",
        "176,180c175,179\r\n",
        "<   name: \"fc7-conv\"\r\n",
        "<   type: CONVOLUTION\r\n",
        "<   bottom: \"fc6-conv\"\r\n",
        "<   top: \"fc7-conv\"\r\n",
        "<   convolution_param {\r\n",
        "---\r\n",
        ">   name: \"fc7\"\r\n",
        ">   type: INNER_PRODUCT\r\n",
        ">   bottom: \"fc6\"\r\n",
        ">   top: \"fc7\"\r\n",
        ">   inner_product_param {\r\n",
        "182d180\r\n",
        "<     kernel_size: 1\r\n",
        "188,189c186,187\r\n",
        "<   bottom: \"fc7-conv\"\r\n",
        "<   top: \"fc7-conv\"\r\n",
        "---\r\n",
        ">   bottom: \"fc7\"\r\n",
        ">   top: \"fc7\"\r\n",
        "194,195c192,193\r\n",
        "<   bottom: \"fc7-conv\"\r\n",
        "<   top: \"fc7-conv\"\r\n",
        "---\r\n",
        ">   bottom: \"fc7\"\r\n",
        ">   top: \"fc7\"\r\n",
        "201,205c199,203\r\n",
        "<   name: \"fc8-conv\"\r\n",
        "<   type: CONVOLUTION\r\n",
        "<   bottom: \"fc7-conv\"\r\n",
        "<   top: \"fc8-conv\"\r\n",
        "<   convolution_param {\r\n",
        "---\r\n",
        ">   name: \"fc8\"\r\n",
        ">   type: INNER_PRODUCT\r\n",
        ">   bottom: \"fc7\"\r\n",
        ">   top: \"fc8\"\r\n",
        ">   inner_product_param {\r\n",
        "207d204\r\n",
        "<     kernel_size: 1\r\n",
        "213c210\r\n",
        "<   bottom: \"fc8-conv\"\r\n",
        "---\r\n",
        ">   bottom: \"fc8\"\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The only differences needed in the architecture are to change the fully-connected classifier inner product layers into convolutional layers with the right filter size -- 6 x 6, since the reference model classifiers take the 36 elements of `pool5` as input -- and stride 1 for dense classification. Note that the layers are renamed so that Caffe does not try to blindly load the old parameters when it maps layer names to the pretrained model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import caffe\n",
      "import matplotlib.pyplot as plt\n",
      "# Load the original network and extract the fully-connected layers' parameters.\n",
      "net = caffe.Net('imagenet/imagenet_deploy.prototxt', 'imagenet/caffe_reference_imagenet_model')\n",
      "params = ['fc6', 'fc7', 'fc8']\n",
      "# fc_params = {name: (weights, biases)}\n",
      "fc_params = {pr: (net.params[pr][0].data, net.params[pr][1].data) for pr in params}\n",
      "\n",
      "for fc in params:\n",
      "    print '{} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "fc6 weights are (1, 1, 4096, 9216) dimensional and biases are (1, 1, 1, 4096) dimensional\n",
        "fc7 weights are (1, 1, 4096, 4096) dimensional and biases are (1, 1, 1, 4096) dimensional\n",
        "fc8 weights are (1, 1, 1000, 4096) dimensional and biases are (1, 1, 1, 1000) dimensional\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the shapes of the inner product parameters. For weights and biases the zeroth and first dimensions are both 1. The second and third weight dimensions are the output and input sizes while the last bias dimension is the output size."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the fully-convolutional network to transplant the parameters.\n",
      "net_full_conv = caffe.Net('imagenet/imagenet_full_conv.prototxt', 'imagenet/caffe_reference_imagenet_model')\n",
      "params_full_conv = ['fc6-conv', 'fc7-conv', 'fc8-conv']\n",
      "# conv_params = {name: (weights, biases)}\n",
      "conv_params = {pr: (net_full_conv.params[pr][0].data, net_full_conv.params[pr][1].data) for pr in params_full_conv}\n",
      "\n",
      "for conv in params_full_conv:\n",
      "    print '{} weights are {} dimensional and biases are {} dimensional'.format(conv, conv_params[conv][0].shape, conv_params[conv][1].shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "fc6-conv weights are (4096, 256, 6, 6) dimensional and biases are (1, 1, 1, 4096) dimensional\n",
        "fc7-conv weights are (4096, 4096, 1, 1) dimensional and biases are (1, 1, 1, 4096) dimensional\n",
        "fc8-conv weights are (1000, 4096, 1, 1) dimensional and biases are (1, 1, 1, 1000) dimensional\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The convolution weights are arranged in output $\\times$ input $\\times$ height $\\times$ width dimensions. To map the inner product weights to convolution filters, we need to roll the flat inner product vectors into channel $\\times$ height $\\times$ width filter matrices.\n",
      "\n",
      "The biases are identical to those of the inner product -- let's transplant these first since no reshaping is needed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for pr, pr_conv in zip(params, params_full_conv):\n",
      "    conv_params[pr_conv][1][...] = fc_params[pr][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The output channels have the leading dimension of both the inner product and convolution weights, so the parameters are translated by reshaping the flat input dimensional parameter vector from the inner product into the channel $\\times$ height $\\times$ width filter shape."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for pr, pr_conv in zip(params, params_full_conv):\n",
      "    out, in_, h, w = conv_params[pr_conv][0].shape\n",
      "    W = fc_params[pr][0].reshape((out, in_, h, w))\n",
      "    conv_params[pr_conv][0][...] = W"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, save the new model weights."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net_full_conv.save('imagenet/caffe_imagenet_full_conv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To conclude, let's make a classification map from the example cat image. This gives an 8-by-8 labeling of overlapping image regions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load input and configure preprocessing\n",
      "im = caffe.io.load_image('images/th-photo3.jpg')\n",
      "plt.imshow(im)\n",
      "net_full_conv.set_mean('data', '../python/caffe/imagenet/ilsvrc_2012_mean.npy')\n",
      "net_full_conv.set_channel_swap('data', (2,1,0))\n",
      "net_full_conv.set_input_scale('data', 255.0)\n",
      "# make classification map by forward pass and show top prediction index per location\n",
      "out = net_full_conv.forward_all(data=np.asarray([net_full_conv.preprocess('data', im)]))\n",
      "out['prob'][0].argmax(axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "array([[610, 399, 906, 906, 834, 457, 743, 583],\n",
        "       [841, 610, 841, 906, 906, 834, 834, 887],\n",
        "       [749, 610, 911, 841, 841, 841, 834, 713],\n",
        "       [749, 841, 906, 841, 906, 399, 797, 713],\n",
        "       [906, 906, 399, 610, 841, 841, 553, 713],\n",
        "       [749, 834, 885, 501, 841, 797, 553, 713],\n",
        "       [749, 514, 728, 414, 697, 514, 480, 651],\n",
        "       [749, 794, 438, 906, 834, 834, 811, 553]])"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The classifications include various cats -- 282 = tabby, 283 = tiger, 281 = persian -- and foxes and other mammals.\n",
      "\n",
      "In this way the fully-connected layers can be extracted as dense features across an image (see `net_full_conv.blobs['fc6'].data` for instance), which is perhaps more useful than the classification map itself."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*A thank you to Rowland Depp for first suggesting this trick.*"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
